{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ccf82d3",
   "metadata": {},
   "source": [
    "# Graphing Revenge: A Network Analysis of ‘The Count of Monte Cristo’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b68d6",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ca760",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li><b>Preliminary information about data</b></li>\n",
    "        <ol>a. Scope of the project</ol>\n",
    "        <ol>b. Dependencies</ol>\n",
    "    <li><b>Data extraction and data cleaning</b></li>\n",
    "        <ol>Import novel sections</ol>\n",
    "        <ol>Tokenisation and characters extraction</ol>\n",
    "    <li><b>Construction of a network based on co-occurrence</b></li>\n",
    "        <ol></ol>\n",
    "    <li><b>Defining the Narrative Window</b></li>\n",
    "        <ol>Collinear window strategy</ol>\n",
    "        <ol>Coplanar co-occurrence window strategy</ol>\n",
    "    <li><b>Results</b></li>\n",
    "        <ol></ol>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce98316",
   "metadata": {},
   "source": [
    "# 1. Preliminary information about data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3f090",
   "metadata": {},
   "source": [
    "## Scope of the project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5229b7b3",
   "metadata": {},
   "source": [
    "This study deals with the network analysis of Alexandre Dumas’ novel <b>‘The Count of Monte Cristo’</b>, published between 1844 and 1846 in serialized format.This analysis employes the detection of character interactions within the novel based on co-occurrence to construct social network for each of the five volumes of the novel, coherent with the version provided by <a href=\"https://www.gutenberg.org/ebooks/1184\" target=\"_blank\">Project Gutenberg</a>.\n",
    "The network analysis aims at exploring the novel's core themes of <b>revenge and redemption</b>, by observing how they are reflected in the character networks. To this extent, it aims to show how the social network of Dumas’ novel favors Dan-tès’ plan of revenge, uncovering the most relevant characters and the evolution of their centrali-ty compared to their relationship with Edmond throughout the novel. The related article is available in the dedicated <a href=\"\" target=\"_blank\">GitHub Repository</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe499c7d",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc5219a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nameparser in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (1.1.3)\n",
      "Requirement already satisfied: nltk in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: click in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: mplcursors in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (0.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.7.1,>=3.1 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from mplcursors) (3.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from matplotlib!=3.7.1,>=3.1->mplcursors) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from matplotlib!=3.7.1,>=3.1->mplcursors) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from matplotlib!=3.7.1,>=3.1->mplcursors) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from matplotlib!=3.7.1,>=3.1->mplcursors) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from matplotlib!=3.7.1,>=3.1->mplcursors) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from matplotlib!=3.7.1,>=3.1->mplcursors) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from matplotlib!=3.7.1,>=3.1->mplcursors) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from matplotlib!=3.7.1,>=3.1->mplcursors) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from matplotlib!=3.7.1,>=3.1->mplcursors) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.7.1,>=3.1->mplcursors) (1.16.0)\n",
      "Collecting watermark\n",
      "  Downloading watermark-2.4.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: ipython>=6.0 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from watermark) (8.14.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from watermark) (4.11.3)\n",
      "Requirement already satisfied: setuptools in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from watermark) (69.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from importlib-metadata>=1.4->watermark) (3.11.0)\n",
      "Requirement already satisfied: backcall in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from ipython>=6.0->watermark) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from ipython>=6.0->watermark) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from ipython>=6.0->watermark) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from ipython>=6.0->watermark) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from ipython>=6.0->watermark) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from ipython>=6.0->watermark) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from ipython>=6.0->watermark) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from ipython>=6.0->watermark) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from ipython>=6.0->watermark) (5.9.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from ipython>=6.0->watermark) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from ipython>=6.0->watermark) (0.1.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.0->watermark) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.0->watermark) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.0->watermark) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from stack-data->ipython>=6.0->watermark) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from stack-data->ipython>=6.0->watermark) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from stack-data->ipython>=6.0->watermark) (0.2.2)\n",
      "Requirement already satisfied: six in /Users/alessandrafailla/anaconda3/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.0->watermark) (1.16.0)\n",
      "Downloading watermark-2.4.3-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: watermark\n",
      "Successfully installed watermark-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nameparser\n",
    "!pip install nltk\n",
    "!pip install mplcursors\n",
    "!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a91ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from statistics import mode, median, mean\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "import plotly.express as px\n",
    "import ALL_regex_1_20, ALL_regex_21_40, ALL_regex_41_85_88_117, ALL_regex_86_87\n",
    "import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d25c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "networkx  : 2.8.4\n",
      "nltk      : 3.7\n",
      "mplcursors: 0.5.3\n",
      "plotly    : 5.9.0\n",
      "re        : 2.2.1\n",
      "matplotlib: 3.7.0\n",
      "watermark : 2.4.3\n",
      "numpy     : 1.23.5\n",
      "pandas    : 1.5.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317c3b9",
   "metadata": {},
   "source": [
    "# 2. Data extraction and data cleaning\n",
    "After downloading the text file of the novel from Project Gutenberg, it was divided into <b>eight sections</b> to allow a more precise identification of character names for the substitution with a unique name using regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b70743",
   "metadata": {},
   "source": [
    "## Import novel sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d1524",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# VOLUME 1\n",
    "with open(\"/Users/alessandrafailla/Desktop/Network_analysis_test/montecristo_chapter_division2/ch1_20_countmontecristo.txt\") as f:\n",
    "    montecristo_1_20=f.read()\n",
    "    \n",
    "with open(\"/Users/alessandrafailla/Desktop/Network_analysis_test/montecristo_chapter_division2/ch21_27_countmontecristo.txt\") as f1:\n",
    "    montecristo_21_27=f1.read()\n",
    "    \n",
    "# VOLUME 2    \n",
    "with open(\"/Users/alessandrafailla/Desktop/Network_analysis_test/montecristo_chapter_division2/ch28_40_countmontecristo.txt\") as f2:\n",
    "    montecristo_28_40=f2.read()\n",
    "    \n",
    "with open(\"/Users/alessandrafailla/Desktop/Network_analysis_test/montecristo_chapter_division2/ch41_47_countmontecristo.txt\") as f2a:\n",
    "    montecristo_41_47=f2a.read()\n",
    "\n",
    "# VOLUME 3 \n",
    "with open(\"/Users/alessandrafailla/Desktop/Network_analysis_test/montecristo_chapter_division2/ch48_73_countmontecristo.txt\") as f3:\n",
    "    montecristo_48_73=f3.read()\n",
    "\n",
    "# VOLUME 4\n",
    "with open(\"/Users/alessandrafailla/Desktop/Network_analysis_test/montecristo_chapter_division2/ch74_85_countmontecristo.txt\") as f4:\n",
    "    montecristo_74_85=f4.read()\n",
    "    \n",
    "with open(\"/Users/alessandrafailla/Desktop/Network_analysis_test/montecristo_chapter_division2/ch86_87_countmontecristo.txt\") as f4a:\n",
    "    montecristo_86_87=f4a.read()\n",
    "    \n",
    "with open(\"/Users/alessandrafailla/Desktop/Network_analysis_test/montecristo_chapter_division2/ch88_95_countmontecristo.txt\") as f4b:\n",
    "    montecristo_88_95=f4b.read()\n",
    "    \n",
    "# VOLUME 5\n",
    "with open(\"/Users/alessandrafailla/Desktop/Network_analysis_test/montecristo_chapter_division2/ch96_117_countmontecristo.txt\") as f5:\n",
    "    montecristo_96_117=f5.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12355225",
   "metadata": {},
   "source": [
    "Names of characters are researched within each section to be replaced with unique IDs. <b>Regular expressions</b> for a total of 46 characters were developed, taking into consideration all possible name variations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in ALL_regex_1_20.regex_dict.items():\n",
    "    montecristo_1_20 = re.sub(key, value, montecristo_1_20)\n",
    "    \n",
    "for key, value in ALL_regex_21_40.regex_dict.items():\n",
    "    montecristo_21_27 = re.sub(key, value, montecristo_21_27)\n",
    "    montecristo_28_40 = re.sub(key, value, montecristo_28_40)\n",
    "    \n",
    "for key, value in ALL_regex_41_85_88_117.regex_dict.items():\n",
    "    montecristo_41_47 = re.sub(key, value, montecristo_41_47)\n",
    "    montecristo_48_73 = re.sub(key, value, montecristo_48_73)\n",
    "    montecristo_74_85 = re.sub(key, value, montecristo_74_85)\n",
    "    montecristo_88_95 = re.sub(key, value, montecristo_88_95)\n",
    "    montecristo_96_117 = re.sub(key, value, montecristo_96_117) \n",
    "\n",
    "for key, value in ALL_regex_86_87.regex_dict.items():\n",
    "    montecristo_86_87 = re.sub(key, value, montecristo_86_87)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5016b3c",
   "metadata": {},
   "source": [
    "Each section containing IDs for each occurrence of characters in the text is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a146068",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1_20 = \"/Users/alessandrafailla/Desktop/Network_analysis_test/definitive_sub_chapters/definitive_montecristo_1_20.txt\"\n",
    "file_path_21_27 = \"/Users/alessandrafailla/Desktop/Network_analysis_test/definitive_sub_chapters/definitive_montecristo_21_27.txt\"\n",
    "file_path_28_40 = \"/Users/alessandrafailla/Desktop/Network_analysis_test/definitive_sub_chapters/definitive_montecristo_28_40.txt\"\n",
    "file_path_41_47 = \"/Users/alessandrafailla/Desktop/Network_analysis_test/definitive_sub_chapters/definitive_montecristo_41_47.txt\"\n",
    "file_path_48_73 = \"/Users/alessandrafailla/Desktop/Network_analysis_test/definitive_sub_chapters/definitive_montecristo_48_73.txt\"\n",
    "file_path_74_85 = \"/Users/alessandrafailla/Desktop/Network_analysis_test/definitive_sub_chapters/definitive_montecristo_74_85.txt\"\n",
    "file_path_86_87 = \"/Users/alessandrafailla/Desktop/Network_analysis_test/definitive_sub_chapters/definitive_montecristo_86_87.txt\"\n",
    "file_path_88_95 = \"/Users/alessandrafailla/Desktop/Network_analysis_test/definitive_sub_chapters/definitive_montecristo_88_95.txt\"\n",
    "file_path_96_117 = \"/Users/alessandrafailla/Desktop/Network_analysis_test/definitive_sub_chapters/definitive_montecristo_96_117.txt\"\n",
    "\n",
    "with open(file_path_1_20, 'w') as file1:\n",
    "    file1.write(montecristo_1_20)\n",
    "\n",
    "with open(file_path_21_27, 'w') as file2:\n",
    "    file2.write(montecristo_21_27)\n",
    "\n",
    "with open(file_path_28_40, 'w') as file3:\n",
    "    file3.write(montecristo_28_40)\n",
    "\n",
    "with open(file_path_41_47, 'w') as file4:\n",
    "    file4.write(montecristo_41_47)\n",
    "\n",
    "with open(file_path_48_73, 'w') as file5:\n",
    "    file5.write(montecristo_48_73)\n",
    "\n",
    "with open(file_path_74_85, 'w') as file6:\n",
    "    file6.write(montecristo_74_85)\n",
    "\n",
    "with open(file_path_86_87, 'w') as file7:\n",
    "    file7.write(montecristo_86_87)\n",
    "\n",
    "with open(file_path_88_95, 'w') as file8:\n",
    "    file8.write(montecristo_88_95)\n",
    "    \n",
    "with open(file_path_96_117, 'w') as file9:\n",
    "    file9.write(montecristo_96_117)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c0845",
   "metadata": {},
   "source": [
    "## Tokenisation and characters extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ded309",
   "metadata": {},
   "source": [
    "### Full text tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556cff49",
   "metadata": {},
   "source": [
    "The novel, currently in string format, is converted into a list of tokens using the **nltk** (Natural Language Toolkit) library. Sections are joined to obtain the full text version of the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2fd0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordTokens(text):\n",
    "    wtokens = nltk.word_tokenize(text.lower())\n",
    "    wtokens = [w for w in wtokens if w not in '!\"#$%&\\'()*+, -./:;<=>?@[\\]^_`{|}~”“’—‘']\n",
    "    return wtokens\n",
    "\n",
    "count_montecristo_full_final = montecristo_1_20 + montecristo_21_27 + montecristo_28_40 + montecristo_41_47 + montecristo_48_73 + montecristo_74_85 + montecristo_86_87 + montecristo_88_95 + montecristo_96_117\n",
    "tokens_count_montecristo = wordTokens(count_montecristo_full_final.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651bda7",
   "metadata": {},
   "source": [
    "### Novel volumes tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c13a94",
   "metadata": {},
   "source": [
    "Sections are joined to provide the five volumes of the novel. Then, each volume is tokenised to obtain a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "montecristo_section_1 = montecristo_1_20 + montecristo_21_27\n",
    "montecristo_section_2 = montecristo_28_40 + montecristo_41_47\n",
    "montecristo_section_3 = montecristo_48_73\n",
    "montecristo_section_4 = montecristo_74_85 + montecristo_86_87 + montecristo_88_95\n",
    "montecristo_section_5 = montecristo_96_117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6271164",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_montecristo_section_1 = wordTokens(montecristo_section_1.lower())\n",
    "tokens_montecristo_section_2 = wordTokens(montecristo_section_2.lower())\n",
    "tokens_montecristo_section_3 = wordTokens(montecristo_section_3.lower())\n",
    "tokens_montecristo_section_4 = wordTokens(montecristo_section_4.lower())\n",
    "tokens_montecristo_section_5 = wordTokens(montecristo_section_5.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845c5131",
   "metadata": {},
   "source": [
    "### Extract characters and character indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0dff6d",
   "metadata": {},
   "source": [
    "Creation of a list and a dictionary containing character names extracted from the regular expression dictionary. The character dictionary contains names as keys and empty dictionaries as values. Additionally, we extract character indices within the tokenised text, that will be used to find co-occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b454eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace character occurrences with unique character IDs\n",
    "\n",
    "char_dict={}\n",
    "char_list = []\n",
    "\n",
    "for char1 in ALL_regex_1_20.regex_dict.values():\n",
    "    char_dict[char1.lower()]={}\n",
    "\n",
    "for char2 in ALL_regex_21_40.regex_dict.values():\n",
    "    if char2.lower() not in char_dict:\n",
    "        char_dict[char2.lower()]={}\n",
    "        \n",
    "for char3 in ALL_regex_41_85_88_117.regex_dict.values():\n",
    "    if char3.lower() not in char_dict:\n",
    "        char_dict[char3.lower()]={}\n",
    "\n",
    "for char4 in ALL_regex_86_87.regex_dict.values():\n",
    "    if char4.lower() not in char_dict:\n",
    "        char_dict[char4.lower()]={}\n",
    "\n",
    "\n",
    "for char in char_dict.keys():\n",
    "    char_list.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c56fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary containing indices of character mentions within the tokenised text\n",
    "def position_dict(chars, my_words):\n",
    "    pos_dict = {}\n",
    "    for i in chars.keys():\n",
    "        k = []\n",
    "        for ix, j in enumerate(my_words):\n",
    "            if j == i.lower():\n",
    "                k.append(ix)\n",
    "        pos_dict[i] = np.array(k)\n",
    "    return pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d426891",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_dict = position_dict(char_dict, tokens_count_montecristo)\n",
    "indices_dict_vol1 = position_dict(char_dict, tokens_montecristo_section_1)\n",
    "indices_dict_vol2 = position_dict(char_dict, tokens_montecristo_section_2)\n",
    "indices_dict_vol3 = position_dict(char_dict, tokens_montecristo_section_3)\n",
    "indices_dict_vol4 = position_dict(char_dict, tokens_montecristo_section_4)\n",
    "indices_dict_vol5 = position_dict(char_dict, tokens_montecristo_section_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41295d",
   "metadata": {},
   "source": [
    "# 3. Construction of network based on co-occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb9fa4",
   "metadata": {},
   "source": [
    "The functions defined in this section allow to build a characters interaction dictionary based on **co-occurrence** of character mentions in the tokenised novel. After removing characters with less than 3 interactions, we create a list of tuples, each tuple representing the interaction between two characters. We provide a method to merge the results of interaction detection retrieved through collinear and co-planar method, to integrate interactions detected only through collinear approach. Finally, we provide a **graph constructor** that provides a graph starting from the list of tuples representing interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47780bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary containing links among characters, given a distance threshold (narrative window)\n",
    "# Characters' IDs are used as keys, while nested dictionaries containing names of characters and\n",
    "# number of interactions are the values. Characters without any interaction are removed\n",
    "def links_dic_f(indices_dic, threshold):\n",
    "    link_dic = {}\n",
    "    rem_set = set()\n",
    "    for first_char, ind_arr1 in indices_dic.items():\n",
    "        dic = {}\n",
    "        for second_char, ind_arr2 in indices_dic.items():\n",
    "            if first_char == second_char:\n",
    "                continue\n",
    "            matr = np.abs(ind_arr1[np.newaxis].T - ind_arr2) <= threshold\n",
    "            s = np.sum(matr)\n",
    "            if s > 3:\n",
    "                dic[second_char] = s\n",
    "        link_dic[first_char] = dic\n",
    "    \n",
    "    for key in link_dic:\n",
    "        if link_dic[key] == {}:\n",
    "            rem_set.add(key)\n",
    "\n",
    "    for key in rem_set:\n",
    "        del link_dic[key]\n",
    "\n",
    "    return link_dic\n",
    "\n",
    "\n",
    "# Returns a list of tuples that represent the connection between characters (the edges of the graph)\n",
    "def edge_tuples_f(link_dic):\n",
    "    edges_tuples = []\n",
    "    for key in link_dic:\n",
    "        for item, value in link_dic[key].items():\n",
    "            tup = (key, item, value)\n",
    "            edges_tuples.append(tup)\n",
    "            \n",
    "    return edges_tuples\n",
    "\n",
    "\n",
    "# Adds nodes if they don't exist already, adds edge between characters and the number of interactions\n",
    "def add_nodes_and_weighted_edges_from_list(graph, edges_list):\n",
    "    for edge in edges_list:\n",
    "        character1, character2, num_interactions = edge\n",
    "        if character1 not in graph:\n",
    "            graph.add_node(character1)\n",
    "        if character2 not in graph:\n",
    "            graph.add_node(character2)\n",
    "        graph.add_edge(character1, character2, weight=num_interactions)\n",
    "        graph.add_edge(character2, character1, weight=num_interactions)  # Add edge in both directions\n",
    "\n",
    "    return graph\n",
    "\n",
    "def update_combined_window(collinear_interactions_dict, coplanar_interact_dict):\n",
    "    for key in collinear_interactions_dict.keys():\n",
    "        if key not in coplanar_interact_dict:\n",
    "            coplanar_interact_dict[key]=collinear_interactions_dict[key]\n",
    "        else:\n",
    "            for val in collinear_interactions_dict[key]:\n",
    "                if val not in coplanar_interact_dict[key]:\n",
    "                    coplanar_interact_dict[key][val] = collinear_interactions_dict[key][val]\n",
    "                    \n",
    "    return coplanar_interact_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb53f60",
   "metadata": {},
   "source": [
    "#  4. Defining the Narrative Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edce1bc9",
   "metadata": {},
   "source": [
    "To perform a network analysis on the novel, we first need to define **criteria to identify an interaction** between two characters in the text. To do so, we need to identify a **narrative window**, a number of words identifying the boundaries within the text that can include one interaction.\n",
    "We rely onto two different approaches to define the narrative window: the **collinear and the co-planar approach**, that will be then used complementarily to find an optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73020af",
   "metadata": {},
   "source": [
    "## Collinear window strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bab413",
   "metadata": {},
   "source": [
    "Applying a **collinear approach** to find the optimal narrative window to identify character interactions consists of searching consequent characters mentions within the text. Thus, given one character, we identify their interaction only with characters mentioned consequently within the window.\n",
    "We build dictionaries of collinear interactions for **narrative windows ranging from 10 to 500**. We then compute the **edge density** for each interaction dictionary and plot the resulting values, with edge density on the y axis and the window size on the x axis. Finally, we identify the narrative window as the value on the x axis where the plot starts to flatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc1925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coll_appr(text, char_lst, window):\n",
    "    dict_interazioni ={}\n",
    "    for ix, i in enumerate(text):\n",
    "        if i in char_lst:\n",
    "            end = min(ix+window, len(text))\n",
    "            for j in text[ix+1:end]:\n",
    "                if j in char_lst and j != i:\n",
    "                    if i not in dict_interazioni:\n",
    "                        dict_interazioni[i] = {j: 1}\n",
    "                        break\n",
    "                    else:\n",
    "                        if j not in dict_interazioni[i]:\n",
    "                            dict_interazioni[i][j] = 1\n",
    "                            break\n",
    "                        else:\n",
    "                            dict_interazioni[i][j] += 1\n",
    "                            break\n",
    "\n",
    "    # Remove interactions occurring less than 3 times\n",
    "    interactions_to_remove = []\n",
    "    for char, interactions in dict_interazioni.items():\n",
    "        for partner, count in interactions.items():\n",
    "            if count < 3:\n",
    "                interactions_to_remove.append((char, partner))\n",
    "    for char, partner in interactions_to_remove:\n",
    "        del dict_interazioni[char][partner]\n",
    "                            \n",
    "    return dict_interazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collinear_window_iterations(window_min, window_max, tokens, char_list):\n",
    "    dict_interactions_coll = {}\n",
    "    dict_of_graphs = {}\n",
    "    range_of_windows = range(window_min, window_max)\n",
    "    for number in range_of_windows:\n",
    "        G = nx.Graph()\n",
    "        coll_interactions_link = coll_appr(tokens, char_list, number)\n",
    "        dict_interactions_coll[number] = coll_interactions_link\n",
    "        G = add_nodes_and_weighted_edges_from_list(G, edge_tuples_f(coll_interactions_link))\n",
    "        dict_of_graphs[number] = G\n",
    "    return dict_interactions_coll, dict_of_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d76277",
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_dict_interactions, dict_Gfull_collinear = collinear_window_iterations(10, 500, tokens_count_montecristo, char_list)\n",
    "coll_dict1_interactions, dict_Gvol1_collinear = collinear_window_iterations(10, 500, tokens_montecristo_section_1, char_list)\n",
    "coll_dict2_interactions, dict_Gvol2_collinear = collinear_window_iterations(10, 500, tokens_montecristo_section_2, char_list)\n",
    "coll_dict3_interactions, dict_Gvol3_collinear = collinear_window_iterations(10, 500, tokens_montecristo_section_3, char_list)\n",
    "coll_dict4_interactions, dict_Gvol4_collinear = collinear_window_iterations(10, 500, tokens_montecristo_section_4, char_list)\n",
    "coll_dict5_interactions, dict_Gvol5_collinear = collinear_window_iterations(10, 500, tokens_montecristo_section_5, char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c0a66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate and return the density of a graph \n",
    "def calculate_density(graph):\n",
    "    num_nodes = 46\n",
    "    num_edges = len(graph.edges)\n",
    "\n",
    "    if num_nodes <= 1:\n",
    "        return 0.0 \n",
    "\n",
    "    density = (2.0 * num_edges) / (num_nodes * (num_nodes - 1))\n",
    "\n",
    "    return density\n",
    "\n",
    "# Plot edge density to find the value at which the plot plateaus\n",
    "def plot_edge_density(dict_graphs, flat_value = 100):\n",
    "    densities = []\n",
    "    threshold_list = []\n",
    "\n",
    "    for threshold, graph in dict_graphs.items():\n",
    "        density = calculate_density(graph)\n",
    "        densities.append(density)\n",
    "        threshold_list.append(threshold)\n",
    "    \n",
    "    # Create a plotly figure\n",
    "    fig = px.line(x=threshold_list, y=densities, labels={'x': 'Threshold', 'y': 'Edge Density'})\n",
    "\n",
    "    # Add interactive hover labels\n",
    "    fig.update_traces(mode='lines+markers', hovertemplate=\"Threshold: %{x}<br>Edge Density: %{y:.4f}\")\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "    delta_density = []\n",
    "    for i,j in zip(densities[1:], densities[:-1]):\n",
    "        delta_density.append(i-j)\n",
    "        \n",
    "        \n",
    "    idx = 0\n",
    "    current_value = 0\n",
    "    while idx < len(delta_density) and not current_value == flat_value:\n",
    "        if delta_density[idx] == 0:\n",
    "            current_value += 1\n",
    "        else:\n",
    "            current_value = 0\n",
    "        idx += 1\n",
    "        \n",
    "        \n",
    "    if current_value == flat_value:\n",
    "        plateau = idx-flat_value+int(threshold_list[0])\n",
    "        \n",
    "        \n",
    "    # Create a plotly figurea\n",
    "    fig = px.line(x=threshold_list[:-1], y=delta_density, labels={'x': 'Threshold', 'y': 'Edge Density'})\n",
    "\n",
    "    # Add interactive hover labels\n",
    "    fig.update_traces(mode='lines+markers', hovertemplate=\"Threshold: %{x}<br>Edge Density: %{y:.4f}\")\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "    \n",
    "    return plateau\n",
    "    \n",
    "# Plot the values of density for the given threshold range\n",
    "#plot_edge_density(dict_of_graphs)\n",
    "plateau_1 = plot_edge_density(dict_Gvol1_collinear)\n",
    "plateau_2 = plot_edge_density(dict_Gvol2_collinear)\n",
    "plateau_3 = plot_edge_density(dict_Gvol3_collinear)\n",
    "plateau_4 = plot_edge_density(dict_Gvol4_collinear)\n",
    "plateau_5 = plot_edge_density(dict_Gvol5_collinear)\n",
    "\n",
    "coll_narr_window = (plateau_1 + plateau_2 + plateau_3 + plateau_4 + plateau_5)/5\n",
    "print(\"Collinear average window size:\", coll_narr_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb3c02",
   "metadata": {},
   "source": [
    "## Coplanar Co-occurrence Window Strategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e73a32b2",
   "metadata": {},
   "source": [
    "The <b>coplanar</b> strategy considers all character mentions within a designated window of text, even if they are not consecutive. It requires a different approach for deriving narrative window sizes due to the continuous increase in edge density with window size increments. Instead, the number of tokens between characters, “gaps”, is examined, treating these gaps as boundaries of character interaction events to generate window sizes based on statistically derived upper limits. Specifically, the <b>interquartile range</b> (iqr = q3 - q1) is used to define the probable upper limits, with any elements outside these limits considered as suspected outliers and left out, resulting in three window sizes for analysis. The smallest discretized window size value, Q3 = 178, was adopted for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gaps(indices_dic):\n",
    "    # Calculate the gaps between consecutive mentions\n",
    "    gaps = []\n",
    "    for character_indices in indices_dic.values():\n",
    "        gaps.extend(np.diff(character_indices))\n",
    "    return gaps\n",
    "\n",
    "def generate_window_sizes(gaps):\n",
    "    # Analyze the distribution of gaps\n",
    "    q1 = np.percentile(gaps, 25)\n",
    "    q3 = np.percentile(gaps, 75)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Define lower and upper bounds for gaps\n",
    "    inf_dg = q1 - 1.5 * iqr\n",
    "    sup_dg = q3 + 1.5 * iqr\n",
    "\n",
    "    # Remove suspected outliers\n",
    "    filtered_gaps = [gap for gap in gaps if inf_dg <= gap <= sup_dg]\n",
    "\n",
    "    # Generate window sizes\n",
    "    wp1 = q3\n",
    "    wp2 = (sup_dg + q3) / 2\n",
    "    wp3 = sup_dg\n",
    "\n",
    "    return wp1, wp2, wp3\n",
    "\n",
    "# Calculate gaps\n",
    "gaps = calculate_gaps(indices_dict)\n",
    "gaps1 = calculate_gaps(indices_dict_vol1)\n",
    "gaps2 = calculate_gaps(indices_dict_vol2)\n",
    "gaps3 = calculate_gaps(indices_dict_vol3)\n",
    "gaps4 = calculate_gaps(indices_dict_vol4)\n",
    "gaps5 = calculate_gaps(indices_dict_vol5)\n",
    "\n",
    "\n",
    "# Generate window sizes\n",
    "window_sizes = generate_window_sizes(gaps)\n",
    "window_sizes1 = generate_window_sizes(gaps1)\n",
    "window_sizes2 = generate_window_sizes(gaps2)\n",
    "window_sizes3 = generate_window_sizes(gaps3)\n",
    "window_sizes4 = generate_window_sizes(gaps4)\n",
    "window_sizes5 = generate_window_sizes(gaps5)\n",
    "\n",
    "\n",
    "print(\"Window Sizes:\", window_sizes)\n",
    "print(\"Window Sizes vol1:\", window_sizes1)\n",
    "print(\"Window Sizes vol2:\", window_sizes2)\n",
    "print(\"Window Sizes vol3:\", window_sizes3)\n",
    "print(\"Window Sizes vol4:\", window_sizes4)\n",
    "print(\"Window Sizes vol5:\", window_sizes5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "definitive_window_wp1 = (window_sizes1[0] + window_sizes2[0] + window_sizes3[0] + window_sizes4[0] + window_sizes5[0])/5\n",
    "definitive_window_wp2 = (window_sizes1[1] + window_sizes2[1] + window_sizes3[1] + window_sizes4[1] + window_sizes5[1])/5\n",
    "definitive_window_wp3 = (window_sizes1[2] + window_sizes2[2] + window_sizes3[2] + window_sizes4[2] + window_sizes5[2])/5\n",
    "\n",
    "definitive_window_wp1, definitive_window_wp2, definitive_window_wp3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5275417",
   "metadata": {},
   "source": [
    "# 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe4d6c",
   "metadata": {},
   "source": [
    "## Graphs constructors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63b561",
   "metadata": {},
   "source": [
    "The following functions construct network graphs to visualize character interactions in a narrative text. The <code>collinear_graph</code> function focuses on collinear relationships, while the <code>coplanar_graph</code> function emphasizes coplanar relationships. Both functions utilize the <code>final_graph</code> function to create and export the graphs for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cacc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph constructor\n",
    "def final_graph(interact_dct, edges, num):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(interact_dct)\n",
    "    G.add_weighted_edges_from(edges)\n",
    "    outfile= \"Graph_\"+num+\".gexf\"\n",
    "    nx.write_gexf(G, outfile)\n",
    "    return G\n",
    "\n",
    "# Collinear graph\n",
    "def collinear_graph(tokens, characters, window, file_name: str):\n",
    "    collinear_interactions = coll_appr(tokens, characters, window)\n",
    "    collinear_edges = edge_tuples_f(collinear_interactions)\n",
    "    collinear_graph = final_graph(collinear_interactions, collinear_edges, file_name)\n",
    "    return collinear_graph\n",
    "\n",
    "# Co-planar graph\n",
    "def coplanar_graph(indices, characters, window, filename: str):\n",
    "    coplanar_interactions = links_dic_f(indices, window)\n",
    "    coplanar_edges = edge_tuples_f(coplanar_interactions)\n",
    "    coplanar_graph = final_graph(coplanar_interactions, coplanar_edges, filename)\n",
    "    return coplanar_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cfd53e",
   "metadata": {},
   "source": [
    "## Centrality measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33695b08",
   "metadata": {},
   "source": [
    "These functions calculate and visualize centrality measures for nodes in a given graph. The <code>calc_centralities</code> function computes four centrality metrics - degree centrality, betweenness centrality, eigenvector centrality, and closeness centrality - for the nodes in the graph and returns them as a DataFrame. The <code>plot_centrality</code> function plots the top 15 nodes ranked by a specified centrality metric and saves the plot as an image file. Finally, the <code>centralities_function</code> combines these two functions to calculate centrality measures, save them to a CSV file, generate centrality plots, and save them as PNG files, all associated with a given graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7801731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_centralities(graph):\n",
    "    \n",
    "    dgc = nx.degree_centrality(graph)\n",
    "    dgc = pd.DataFrame.from_dict(dgc, orient='index', columns=[\"DGC\"])\n",
    "    btc = nx.betweenness_centrality(graph)\n",
    "    btc = pd.DataFrame.from_dict(btc, orient='index', columns=[\"BTC\"])\n",
    "    evc = nx.eigenvector_centrality(graph, weight='weight', max_iter=600)\n",
    "    evc = pd.DataFrame.from_dict(evc, orient='index', columns=[\"EVC\"])\n",
    "    clc = nx.closeness_centrality(graph)\n",
    "    clc = pd.DataFrame.from_dict(clc, orient='index', columns=[\"CLC\"])\n",
    "    df = pd.concat([dgc, btc, evc, clc], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_centrality(centr, df, title, n, col_list):\n",
    "    \n",
    "    ax = plt.subplot(2, 2, n)\n",
    "    s = df.sort_values(centr, ascending=False)[:10]\n",
    "    x = list(s[centr].index)[::-1]\n",
    "    y = list(s[centr])[::-1]\n",
    "    \n",
    "    for i, v in enumerate(y):\n",
    "        bars = ax.barh(x[i], v, color=col_list[n-1])\n",
    "        ax.bar_label(bars, fmt=\"%.2f\", label_type=\"center\")\n",
    "    \n",
    "    plt.title(title, size=22)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(axis='y', length = 0, labelsize=14)\n",
    "\n",
    "    \n",
    "def centralities_function(G, name):\n",
    "    my_centr=calc_centralities(G)\n",
    "    my_centr.to_csv(\"Centr_\"+name+\".csv\")\n",
    "    figfile_centr=\"Centr_\"+name+\".png\"\n",
    "    col_list = [\"peachpuff\", \"plum\", \"orange\", \"CornflowerBlue\"]\n",
    "    fig, ax = plt.subplots(2,2, figsize=(12, 10))\n",
    "    plt.tight_layout(w_pad=15)\n",
    "    plot_centrality(\"DGC\", my_centr, 'Degree Centrality', 1, col_list)\n",
    "    plot_centrality(\"EVC\", my_centr, 'Closeness Centrality', 2, col_list)\n",
    "    plot_centrality(\"BTC\", my_centr, 'Betweeness Centrality', 3, col_list)\n",
    "    plot_centrality(\"EVC\", my_centr, 'Eigenvector Centrality', 4, col_list)\n",
    "    plt.savefig(figfile_centr, dpi=300, bbox_inches='tight')\n",
    "    return my_centr, figfile_centr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da0348",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b572e4",
   "metadata": {},
   "source": [
    "This sections presents the results for each volume of the novel, as well as for the complete novel. Graphs resulting from <b>coplanar</b>, <b>collinear</b>, and <b>combined</b> approaches are provided, as well as a graphical representation of the highest centrality measures for the combined approach, adopted for the analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb440a2",
   "metadata": {},
   "source": [
    "## Volume 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed27cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Interaction dictionaries (collinear and co-planar)\n",
    "coll_int_v1 = coll_appr(tokens_montecristo_section_1, char_list, 227)\n",
    "copl_int_v1_wp1 = links_dic_f(indices_dict_vol1, 178)\n",
    "\n",
    "# Collinear edges and graph\n",
    "coll_edges_v1 = edge_tuples_f(coll_int_v1)\n",
    "collinear_graph_vol1 = collinear_graph(tokens_montecristo_section_1, char_list, 227, \"_collinear_vol1\")\n",
    "\n",
    "# Co-planar graph for the smallest coplanar window size\n",
    "coplanar_graph_vol1_wp1 = coplanar_graph(indices_dict_vol1, char_dict, 178, \"_coplanar_vol1_wp1\")\n",
    "\n",
    "# Interaction dictionary from integration of coplanar and collinear, for the smallest coplanar window size\n",
    "combined_v1_wp1 = update_combined_window(coll_int_v1, copl_int_v1_wp1)\n",
    "\n",
    "# Edges and graph for combined strategy, for the smallest coplanar window size\n",
    "combined_edges_v1_wp1 = edge_tuples_f(combined_v1_wp1)\n",
    "\n",
    "combined_graph_v1_wp1 = final_graph(combined_v1_wp1, combined_edges_v1_wp1, \"v1_combined_1_20\")\n",
    "\n",
    "# Centrality measures for combined strategy, for the smallest coplanar window size\n",
    "combined_centr_v1_wp1 = calc_centralities(combined_graph_v1_wp1)\n",
    "top_combined_centr_v1_wp1 = centralities_function(combined_graph_v1_wp1, 'v1_combined_1_20')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c7af5e",
   "metadata": {},
   "source": [
    "## Volume 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba062437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction dictionaries (collinear and co-planar)\n",
    "coll_int_v2 = coll_appr(tokens_montecristo_section_2, char_list, 227)\n",
    "copl_int_v2_wp1 = links_dic_f(indices_dict_vol2, 178)\n",
    "\n",
    "# Collinear edges and graph\n",
    "coll_edges_v2 = edge_tuples_f(coll_int_v2)\n",
    "collinear_graph_vol2 = collinear_graph(tokens_montecristo_section_2, char_list, 227, \"_collinear_vol2\")\n",
    "\n",
    "# Co-planar graph for the smallest coplanar window size\n",
    "coplanar_graph_vol2_wp1 = coplanar_graph(indices_dict_vol2, char_dict, 178, \"_coplanar_vol2_wp1\")\n",
    "\n",
    "# Interaction dictionary from integration of coplanar and collinear, for the smallest coplanar window size\n",
    "combined_v2_wp1 = update_combined_window(coll_int_v2, copl_int_v2_wp1)\n",
    "\n",
    "# Edges and graph for combined strategy, for the smallest coplanar window size\n",
    "combined_edges_v2_wp1 = edge_tuples_f(combined_v2_wp1)\n",
    "\n",
    "combined_graph_v2_wp1 = final_graph(combined_v2_wp1, combined_edges_v2_wp1, \"v2_combined_1_20\")\n",
    "\n",
    "# Centrality measures for combined strategy, for the smallest coplanar window size\n",
    "combined_centr_v2_wp1 = calc_centralities(combined_graph_v2_wp1)\n",
    "top_combined_centr_v2_wp1 = centralities_function(combined_graph_v2_wp1, 'v2_combined_1_20')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf64336",
   "metadata": {},
   "source": [
    "## Volume 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction dictionaries (collinear and co-planar)\n",
    "coll_int_v3 = coll_appr(tokens_montecristo_section_3, char_list, 227)\n",
    "copl_int_v3_wp1 = links_dic_f(indices_dict_vol3, 178)\n",
    "\n",
    "# Collinear edges and graph\n",
    "coll_edges_v3 = edge_tuples_f(coll_int_v3)\n",
    "collinear_graph_vol3 = collinear_graph(tokens_montecristo_section_3, char_list, 227, \"_collinear_vol3\")\n",
    "\n",
    "# Co-planar graph for the smallest coplanar window size\n",
    "coplanar_graph_vol3_wp1 = coplanar_graph(indices_dict_vol3, char_dict, 178, \"_coplanar_vol3_wp1\")\n",
    "\n",
    "# Interaction dictionary from integration of coplanar and collinear, for the smallest coplanar window size\n",
    "combined_v3_wp1 = update_combined_window(coll_int_v3, copl_int_v3_wp1)\n",
    "\n",
    "# Edges and graph for combined strategy, for the smallest coplanar window size\n",
    "combined_edges_v3_wp1 = edge_tuples_f(combined_v3_wp1)\n",
    "combined_graph_v3_wp1 = final_graph(combined_v3_wp1, combined_edges_v3_wp1, \"v3_combined_1_20\")\n",
    "\n",
    "# Centrality measures for combined strategy, for the smallest coplanar window size\n",
    "combined_centr_v3_wp1 = calc_centralities(combined_graph_v3_wp1)\n",
    "top_combined_centr_v3_wp1 = centralities_function(combined_graph_v3_wp1, 'v3_combined_1_20')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b65380",
   "metadata": {},
   "source": [
    "## Volume 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cadeea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction dictionaries (collinear and coplanar)\n",
    "coll_int_v4 = coll_appr(tokens_montecristo_section_4, char_list, 227)\n",
    "copl_int_v4_wp1 = links_dic_f(indices_dict_vol4, 178)\n",
    "\n",
    "# Collinear edges and graph\n",
    "coll_edges_v4 = edge_tuples_f(coll_int_v4)\n",
    "collinear_graph_vol4 = collinear_graph(tokens_montecristo_section_4, char_list, 227, \"_collinear_vol4\")\n",
    "\n",
    "# Co-planar graph for the smallest coplanar window size\n",
    "coplanar_graph_vol4_wp1 = coplanar_graph(indices_dict_vol4, char_dict, 178, \"_coplanar_vol4_wp1\")\n",
    "\n",
    "# Interaction dictionary from integration of coplanar and collinear, for the smallest coplanar window size\n",
    "combined_v4_wp1 = update_combined_window(coll_int_v4, copl_int_v4_wp1)\n",
    "\n",
    "# Edges and graph for combined strategy, for the smallest coplanar window size\n",
    "combined_edges_v4_wp1 = edge_tuples_f(combined_v4_wp1)\n",
    "combined_graph_v4_wp1 = final_graph(combined_v4_wp1, combined_edges_v4_wp1, \"v4_combined_1_20\")\n",
    "\n",
    "# Centrality measures for combined strategy, for the smallest coplanar window size\n",
    "combined_centr_v4_wp1 = calc_centralities(combined_graph_v4_wp1)\n",
    "top_combined_centr_v4_wp1 = centralities_function(combined_graph_v4_wp1, 'v4_combined_1_20')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194cc88b",
   "metadata": {},
   "source": [
    "## Volume 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4decc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction dictionaries (collinear and co-planar)\n",
    "coll_int_v5 = coll_appr(tokens_montecristo_section_5, char_list, 227)\n",
    "copl_int_v5_wp1 = links_dic_f(indices_dict_vol5, 178)\n",
    "\n",
    "# Collinear edges and graph\n",
    "coll_edges_v5 = edge_tuples_f(coll_int_v5)\n",
    "collinear_graph_vol5 = collinear_graph(tokens_montecristo_section_5, char_list, 227, \"_collinear_vol5\")\n",
    "\n",
    "# Co-planar graph for the smallest coplanar window size\n",
    "coplanar_graph_vol5_wp1 = coplanar_graph(indices_dict_vol5, char_dict, 178, \"_coplanar_vol5_wp1\")\n",
    "\n",
    "# Interaction dictionary from integration of coplanar and collinear, for the smallest coplanar window size\n",
    "combined_v5_wp1 = update_combined_window(coll_int_v5, copl_int_v5_wp1)\n",
    "\n",
    "# Edges and graph for combined strategy, for the smallest coplanar window size\n",
    "combined_edges_v5_wp1 = edge_tuples_f(combined_v5_wp1)\n",
    "combined_graph_v5_wp1 = final_graph(combined_v5_wp1, combined_edges_v5_wp1, \"v5_combined_1_20\")\n",
    "\n",
    "# Centrality measures for combined strategy, for the smallest coplanar window size\n",
    "combined_centr_v5_wp1 = calc_centralities(combined_graph_v5_wp1)\n",
    "top_combined_centr_v5_wp1 = centralities_function(combined_graph_v5_wp1, 'v5_combined_1_20')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813ccc9",
   "metadata": {},
   "source": [
    "## Graph full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction dictionaries (collinear and co-planar)\n",
    "coll_int_full = coll_appr(tokens_count_montecristo, char_list, 227)\n",
    "copl_int_full_wp1 = links_dic_f(indices_dict, 178)\n",
    "\n",
    "# Collinear edges and graph\n",
    "coll_edges_full = edge_tuples_f(coll_int_full)\n",
    "collinear_graph_full = collinear_graph(tokens_count_montecristo, char_list, 227, \"_collinear_full\")\n",
    "\n",
    "# Co-planar graph for the smallest coplanar window size\n",
    "coplanar_graph_full_wp1 = coplanar_graph(indices_dict, char_dict, 178, \"_coplanar_full_wp1\")\n",
    "\n",
    "# Interaction dictionary from integration of coplanar and collinear, for each window size (wp1, wp2)\n",
    "combined_full_wp1 = update_combined_window(coll_int_full, copl_int_full_wp1)\n",
    "\n",
    "# Edges and graph for combined strategy, for the smallest coplanar window size\n",
    "combined_edges_full_wp1 = edge_tuples_f(combined_full_wp1)\n",
    "combined_graph_full_wp1 = final_graph(combined_full_wp1, combined_edges_full_wp1, \"full_combined_1_20\")\n",
    "\n",
    "# Centrality measures for combined strategy, for the smallest coplanar window size\n",
    "combined_centr_full_wp1 = calc_centralities(combined_graph_full_wp1)\n",
    "top_combined_centr_full_wp1 = centralities_function(combined_graph_full_wp1, 'full_combined_1_20')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26cc9de",
   "metadata": {},
   "source": [
    "## Plotting centrality measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bab174",
   "metadata": {},
   "source": [
    "The evolution of  Edmond Dantes' enemies' degree centrality throughout the novel is plotted to provide an additional insight on the development of their role throughout the narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3288d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_centrality(data):\n",
    "    # Extracting data from the table\n",
    "    characters = data.index.tolist()\n",
    "    volumes = data.columns.tolist()\n",
    "    num_volumes = len(volumes)\n",
    "    \n",
    "    # Creating a new graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Adding nodes to the graph\n",
    "    for character in characters:\n",
    "        G.add_node(character)\n",
    "    \n",
    "    # Adding edges with weights based on degree centrality values\n",
    "    for i, volume in enumerate(volumes):\n",
    "        for j, character1 in enumerate(characters):\n",
    "            for k, character2 in enumerate(characters):\n",
    "                if j != k:\n",
    "                    G.add_edge(character1, character2, weight=data.iloc[j, i] + data.iloc[k, i])\n",
    "    \n",
    "    # Plotting the line graph\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for character in characters:\n",
    "        centrality_values = [data.loc[character, volume] for volume in volumes]\n",
    "        plt.plot(range(1, num_volumes + 1), centrality_values, label=character)\n",
    "        \n",
    "    for character in characters:\n",
    "        centrality_values = [data.loc[character, volume] for volume in volumes]\n",
    "        for i, volume in enumerate(volumes):\n",
    "            plt.annotate(str(round(centrality_values[i], 2)), (i + 1, centrality_values[i]), textcoords=\"offset points\", xytext=(0,3), ha='center')\n",
    "\n",
    "\n",
    "    # Customizing plot\n",
    "    plt.xlabel('Volume')\n",
    "    plt.ylabel('Degree Centrality')\n",
    "    plt.title('Degree Centrality of Dantès Enemies across Volumes')\n",
    "    plt.xticks(range(1, num_volumes + 1), volumes)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'v1': [0.60, 0.67, 0.67, 0.73],\n",
    "    'v2': [0.40, 0.37, 0.14, 0.09],\n",
    "    'v3': [0.58, 0.61, 0.39, 0.11],\n",
    "    'v4': [0.60, 0.66, 0.57, 0.29],\n",
    "    'v5': [0.59, 0.56, 0.06, 0.12]\n",
    "}\n",
    "\n",
    "characters = ['Danglars', 'Villefort', 'Fernand', 'Caderousse']\n",
    "\n",
    "df = pd.DataFrame(data, index=characters)\n",
    "\n",
    "# Calling the function with the sample data\n",
    "plot_degree_centrality(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
